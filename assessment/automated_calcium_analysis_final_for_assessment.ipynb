{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a92271c",
   "metadata": {},
   "source": [
    "# Automating Calcium assay data analysis\n",
    "\n",
    "Annotations specific to this run through for assessment are surrounded by ###:\n",
    "\n",
    "###They are not included in the usual code.###\n",
    "\n",
    "## Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3df130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the packages we'll be using:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2b33f",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "I found that I could only import data from my github if i called it in the raw format, here i've done this automatically so that you can copy the URL directly from the github website (or the test data from my README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '' #<--- insert URL here\n",
    "\n",
    "print(\"The URL is:\", url)\n",
    "\n",
    "\n",
    "raw_url = url.replace(\"blob\", \"raw\") # Replacing blob with raw, data doesn't seem to import unless it is in 'raw' format\n",
    "\n",
    "print(\"\\nOur converted raw URL is:\", raw_url)\n",
    "\n",
    "\n",
    "test_dataset = pd.read_csv(raw_url)# Reading in our data\n",
    "\n",
    "print(\"\\nThis is the whole dataset below:\\n\\n\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619ce354",
   "metadata": {},
   "source": [
    "###There are many unwanted columns here### \n",
    "\n",
    "## Tidying up test_dataset\n",
    "\n",
    "The columns of interest are Time (sec) and the R[i] R1 columns.\n",
    "The first R refers to the Region, in this case individual cells.\n",
    "The second R1 is the ratio between W1 and W2, this is what we're interested in and as the ratio has already been calculated for us we no longer need W1 and W2.\n",
    "\n",
    "### Deleting unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac382c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying and deleting R[i] W1 Avg columns\n",
    "Ri_W1_Avg_columns = [] # Creating a list to add the columns too \n",
    "\n",
    "for i in range(1, 51): # Looping through 50 possible columns # Looping through 50 possible columns\n",
    "    column_name = 'R' + str(i) + ' W1 Avg' # Identifying the columns of interest\n",
    "    if column_name in test_dataset.columns: # IF function to ensure the column identified is in the corect dataset\n",
    "        Ri_W1_Avg_columns.append(column_name) # IF it is, adding the coluymn to a list\n",
    "\n",
    "test_dataset = test_dataset.drop(columns=Ri_W1_Avg_columns) # Deleting the columns identifed in the list\n",
    "\n",
    "    \n",
    "# Identifying and deleting R[i] W2 Avg columns\n",
    "Ri_W2_Avg_columns = [] # Creating a list to add the columns too \n",
    "\n",
    "for i in range(1, 51): # Looping through 50 possible columns # Looping through 50 possible columns\n",
    "    column_name = 'R' + str(i) + ' W2 Avg' # Identifying the columns of interest\n",
    "    if column_name in test_dataset.columns: # IF function to ensure the column identified is in the corect dataset\n",
    "        Ri_W2_Avg_columns.append(column_name) # IF it is, adding the coluymn to a list\n",
    "\n",
    "test_dataset = test_dataset.drop(columns=Ri_W2_Avg_columns) # Deleting the columns identifed in the list\n",
    "\n",
    "    \n",
    "# Deleting columns where all the values are NaN\n",
    "nan_columns = test_dataset.columns[test_dataset.isnull().all()].tolist() # Identifying the columns\n",
    "\n",
    "test_dataset = test_dataset.drop(columns=nan_columns) # Dropping them from the DF\n",
    "\n",
    "\n",
    "# Deleting columns that contain only 0 and NaN values\n",
    "\n",
    "#Identifying the columns that contain only 0 or NaN values\n",
    "na_null_columns = test_dataset.columns[test_dataset.isin([0, pd.np.nan]).all() & test_dataset.notnull().any(axis=0)]\n",
    "        \n",
    "\n",
    "test_dataset = test_dataset.drop(columns=na_null_columns) # Dropping them from the DF\n",
    "\n",
    "\n",
    "# Deleting unnamed columns\n",
    "unnamed_columns = [col for col in test_dataset.columns if \"Unnamed:\" in col] # Identifying the unnamed columns\n",
    "\n",
    "test_dataset = test_dataset.drop(columns=unnamed_columns) # Dropping them from the DF\n",
    "\n",
    "\n",
    "print(\" This is the test dataset without: R[i] W1 or W2 Avg columns, NaN columns, blank columns or unnamed columns:\\n\\n\", test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327494da",
   "metadata": {},
   "source": [
    "### Naming our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = test_dataset[\"Time (sec)\"] # Assigning the 'Time (sec)' column the name 'time'\n",
    "\n",
    "\n",
    "# Assigning the R[i] R1 columns the name calcium_r[i]\n",
    "for i in range(1, 51): # Looping through 50 possible columns # Looping through 50 possible columns\n",
    "    try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "        exec(f'calcium_r{i} = test_dataset[\"R{i} R1\"].values') # Assigning names\n",
    "    except KeyError:\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "print(\"Our time dataset looks like this:\\n\\n\", time, \"\\n\\n\\n\\n And an example of our calcium_r[i] (9) dataset looks like this:\\n\\n\", calcium_r9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f85ce6",
   "metadata": {},
   "source": [
    "## Visualising all of our calcium_r[i] data against time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223b47d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Setting the number of rows based on the number of columns that are in the original df\n",
    "num_row = None # Creating a variable\n",
    "\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "    var_name = \"calcium_r\" + str(i) # Creating a variable\n",
    "    if var_name in locals(): # IF function to see if the current iteration of calcium_r[i] is defined\n",
    "        num_row = i # Assigning the highest value to our variable\n",
    "\n",
    "num_row_half = math.ceil(num_row / 2) # Divides the number of rows by 2 (as there are 2 columns) and rounding up\n",
    "\n",
    "\n",
    "# Plotting the data\n",
    "nrows = math.ceil(num_row_half) # Setting no. of rows (#+1 as sometimes column calcium_r1 is not the first column with data)\n",
    "ncols = 2 # Setting no. of columns\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10, nrows*5)) # Creating figure/subplots\n",
    "\n",
    "axs = axs.flatten() # Flattening the subplot array to make it easier to iterate over\n",
    "\n",
    "# Looping through the variables to plot each one\n",
    "for i, ax in enumerate(axs): # Looping through 50 possible columns\n",
    "    calcium_r = \"calcium_r\" + str(i+1) # Calling calcium_r[i] for the current iteration\n",
    "    if calcium_r in locals():\n",
    "        ax.plot(time, locals()[calcium_r], 'b.') # Plotting time on the x axis\n",
    "        ax.set_title(\"Calcium {}/Time\".format(i+1)) # Setting the title\n",
    "        ax.set_ylabel(\"Calcium {}\".format(i+1)) # Setting the y axis label\n",
    "        ax.set_xlabel(\"Time (sec)\") # Setting the x axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264fe1d",
   "metadata": {},
   "source": [
    "## Standardising calcium_r[i] data\n",
    "\n",
    "To be able to compare between cells with varying peaks and baselines I am standardising the output of y on a scale of 0 and 1 by dividing all values by the peak value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the peak of each dataset.\n",
    "for i in range(1, 51):\n",
    "    try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "        dataset = eval(f\"calcium_r{i}\") # Defining the dataset\n",
    "        peak = max(dataset) # Setting the peak to the maximum value in the dataset\n",
    "        exec(f\"peak{i} = {peak}\") # Saving the value as a new peak[i] variable\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "# Dividing each value by the dataset's corresponding peak\n",
    "for i in range(1, 51):\n",
    "    try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "        peak = eval(f\"peak{i}\")  # Set peak value for the current iteration\n",
    "        calcium_r = eval(f\"calcium_r{i}\")  # Get the calcium_r[i] for the current iteration\n",
    "        standardised_calcium_r = [value / peak for value in calcium_r] # Standardise each point of data by dividing by peak\n",
    "        exec(f\"standardised_calcium_r{i} = standardised_calcium_r\") \n",
    "                # Saving the results as a new variable\n",
    "    except NameError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447dc1ff",
   "metadata": {},
   "source": [
    "### Visualising standardised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8449fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the data\n",
    "nrows = math.ceil(num_row_half)  # Setting no. of rows (#+1 as sometimes column calcium_r1 is not the first column with data)\n",
    "ncols = 2  # Setting no. of columns\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10, nrows * 5))  # Creating figure/subplots\n",
    "\n",
    "axs = axs.flatten()  # Flattening the subplot array to make it easier to iterate over\n",
    "\n",
    "# Looping through the variables to plot each one\n",
    "for i, ax in enumerate(axs):  # Looping through 50 possible columns\n",
    "    standardised_calcium_r = f\"standardised_calcium_r{i + 1}\"  # Calling standardised_calcium_r[i] for the current iteration\n",
    "    if standardised_calcium_r in locals():\n",
    "        ax.plot(time, locals()[standardised_calcium_r], \"b.\")  # Plotting time on the x axis\n",
    "        ax.set_title(\"Standardised Calcium {}/Time\".format(i+1)) # Setting the title\n",
    "        ax.set_ylabel(\"Standardised Calcium {}\".format(i+1)) # Setting the y axis label\n",
    "        ax.set_xlabel(\"Time (sec)\") # Setting the x axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305b851",
   "metadata": {},
   "source": [
    "The data seems to consist of swift incline (the action of a drug) which comes to a peak followed by a small lag period and a swift decline. This decline then plateaus to a trough, and finally the data tails off and rises again.\n",
    "I am interested in identifying the steep rate of decline after the lag phase but before the decline begins to plateau - this should be a fairly consistent measure of how quickly calcium levels return to normal.\n",
    "\n",
    "## Identifying the rate at which calcium levels return to normal following the peak\n",
    "\n",
    "This code was designed to call a range of datasets with up to 50 regions/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining standardised_calcium_r[i] and time into a list of tuples\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "    try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "        globals()['time_tuples' + str(i)] = list(zip(time, locals()['standardised_calcium_r' + str(i)])) # Combines the matching indexes\n",
    "                # of time and standardised_calcium_r[i] into tuples\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Calculating the gradient between the tuples\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "    try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "        gradients = [] # Clearing the gradients list for this iteration \n",
    "        for j in range(len(globals()['time_tuples' + str(i)])-1): # Looping through the 50 possible time_tuples\n",
    "            gradient = (globals()['time_tuples' + str(i)][j+1][1] - globals()['time_tuples' + str(i)][j][1]) / (globals()['time_tuples' + str(i)][j+1][0] - globals()['time_tuples' + str(i)][j][0])\n",
    "                    # Defining each gradient\n",
    "            gradients.append(gradient) # Appending each gradient to a list of gradients\n",
    "        globals()['gradients' + str(i)] = gradients # Naming the gradients list with the number of the corresponding\n",
    "                # standardised_calcium_r[i]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Defining a function to find the longest straight(ish) declining line (this should be the line of interest) based on a\n",
    "    # tolerance of 'straightness'.\n",
    "def calculate_average_gradient(times, values, tolerance): # Defining the fucniton and it's arguments\n",
    "  time_tuples = list(zip(times, values)) # \n",
    "  gradients = [] # Clearing the gradients list for this iteration \n",
    "  for i in range(len(time_tuples)-1): # Looping through 50 possible time_tuples\n",
    "    gradient = (time_tuples[i+1][1] - time_tuples[i][1]) / (time_tuples[i+1][0] - time_tuples[i][0]) # Defining the\n",
    "            # gradient between each of the points \n",
    "    gradients.append(gradient) # Adding this gradient to the list of graients\n",
    "  max_length = 0 # Setting our variables to 0\n",
    "  start_index = 0 # Setting our variables to 0\n",
    "  end_index = 0 # Setting our variables to 0\n",
    "  previous_gradient = gradients[0] # Moving on to the next gradient\n",
    "  for i in range(len(time_tuples)-1):\n",
    "    length = 0 # Setting our variables to 0\n",
    "    for j in range(i, len(time_tuples)-1): # Looping through 50 possible time_tuples\n",
    "      average_gradient = (time_tuples[j+1][1] - time_tuples[i][1]) / (time_tuples[j+1][0] - time_tuples[i][0])\n",
    "              # Calculating the average gradient of the line\n",
    "      percentage_change = (gradients[j] - average_gradient) / average_gradient # Determining the percentage change between\n",
    "              # the average gradient before and after adding this gradient\n",
    "      if abs(percentage_change) < tolerance and average_gradient < 0: # Checking that the percentage change is within the\n",
    "              # set tolerance, and that the line has an overall negative gradient\n",
    "        length += 1 # Adds +1 to the length of the line if these conditions are met.\n",
    "        previous_gradient = average_gradient # Moving on to the next gradient\n",
    "      else: # If the above statement fails, this line breaks\n",
    "        break\n",
    "    if length > max_length: # If the line is the largest found so far it is saved\n",
    "      max_length = length\n",
    "      start_index = i\n",
    "      end_index = i+length\n",
    "  times1, values1 = zip(*time_tuples[start_index:end_index+1]) # Fetches the start and end index from the tuples\n",
    "  average_gradient = (values1[-1] - values1[0]) / (times1[-1] - times1[0]) # Calculates the average gradient of the line\n",
    "  return average_gradient, start_index, end_index # Returns the average gradient adn the start and end index of the max line\n",
    "\n",
    "\n",
    "# Setting the tolerances for all standardised_calcium_r[i] to a default 0.3 - seems to work in the majority of cases\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "    if f'standardised_calcium_r{i}' in globals(): # Only sets a tolerance if the standardised_calcium_r[i] is defined\n",
    "        exec(f'tolerance{i} = 0.3') # Sets the tolerance[i] to 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c16a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on all standardised_calcium_r[i] to find the line of interest in each \n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "  try: # Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "    standardised_calcium_r_values = globals()[\"standardised_calcium_r\"+str(i)]  # Get the standardised_calcium_r values for the current iteration\n",
    "    tolerance = globals()[\"tolerance\"+str(i)]  # Get the tolerance value for the current iteration\n",
    "    average_gradient, start_index, end_index = calculate_average_gradient(time, standardised_calcium_r_values, tolerance) #Runs the\n",
    "            # function wiht the arguments unique to each standardised_calcium_r[i]\n",
    "    globals()[\"average_gradient\"+str(i)] = average_gradient\n",
    "    globals()[\"start_index\"+str(i)] = start_index\n",
    "    globals()[\"end_index\"+str(i)] = end_index\n",
    "  except KeyError:\n",
    "    continue\n",
    "\n",
    "# Extracting the start and end index for the lines - this will be useful in plotting them later! This was done in the\n",
    "        # function but were not saved as [i], so were overwritten for each iteration.\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "  try:# Using try and except in the loop to avoid errors relating to missing columns in datasets of less than 50 cells\n",
    "    start_index = globals()[\"start_index\"+str(i)]  # Sets the variable to the one defined in the current iteration [i]\n",
    "    end_index = globals()[\"end_index\"+str(i)]  # Sets the variable to the one defined in the current iteration [i]\n",
    "    time_tuples_values = globals()[\"time_tuples\"+str(i)]  # Sets the variable to the one defined in the current iteration [i]\n",
    "    times, values = zip(*time_tuples_values[start_index:end_index+1]) # Extracts the start and end index\n",
    "    globals()[\"times\"+str(i)] = times # Names start and end index as times[i] and values[i]\n",
    "    globals()[\"values\"+str(i)] = values # Names start and end index as times[i] and values[i]\n",
    "  except KeyError:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2f6b8",
   "metadata": {},
   "source": [
    "## Plotting all of our lines on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51bb9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrows = math.ceil(num_row_half) # Setting no. of rows\n",
    "ncols = 2 # Setting no. of columns\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(10, nrows*5)) # Creating the figure and subplots\n",
    "\n",
    "axs = axs.flatten() # Flattening the subplot array to make it easier to iterate over - unsure if this is required\n",
    "\n",
    "# Looping through the variables to plot each one\n",
    "for i, ax in enumerate(axs): # Looping through 50 possible columns\n",
    "    standardised_calcium_r = \"standardised_calcium_r\" + str(i+1) # Sets the variable to the one defined in the current iteration [i]\n",
    "    values = \"values\" + str(i+1) # Sets the variable to the one defined in the current iteration [i]\n",
    "    times = \"times\" + str(i+1) # Sets the variable to the one defined in the current iteration [i]\n",
    "    if standardised_calcium_r in locals() and values in locals() and times in locals(): # Verifying that the variables are defined first\n",
    "        ax.plot(time, locals()[standardised_calcium_r], 'b.') # Plotting time on the x axis\n",
    "        ax.plot(locals()[times], locals()[values], 'r-') # Plotting our y axis and a red line denoting our max line\n",
    "        ax.set_title(\"R{} calcium over time + fitted line (red)\".format(i+1)) # Setting the title\n",
    "        ax.set_ylabel(\"Calcium level\".format(i+1)) # Setting the y axis label\n",
    "        ax.set_xlabel(\"Time (sec)\") # Setting the x axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f79633",
   "metadata": {},
   "source": [
    "## Making any reqired changes\n",
    "\n",
    "### Refining tolerances\n",
    "\n",
    "Once you've visualised the lines on the plots it's possible to edit the tolerance for each plot to try and improve the fit - this should only be required for a limited number of the plots as the default value works quite well! Increasing the tolerance should allow the line to get larger and work it's way through messier data, if the line is overstretched across the lag region or where the line begins to plateau it may be suitable o decrease the tolerance to improve the fit of the line.\n",
    "\n",
    "Change the tolerance levels of specific standardised_calcium_r[i] by using the following format below:\n",
    "\n",
    "(Do this as many times as required.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###As an example - above the line fitted to standardised_calcium_r13 seems to include the lag period which is visibly\n",
    "#          skewing the gradient.###\n",
    "\n",
    "\n",
    "tolerance12 = 0.4 ### Increaseing the tolerance for 12 will fit a longer line and include more of the region we're after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8630c",
   "metadata": {},
   "source": [
    "Now go back up and run the cell titled '# Running the function...' and the next cell to re-plot the data and check your fit.\n",
    "\n",
    "Either make further adjustments or continue on to calculate the overall average gradient for this data sheet.\n",
    "   \n",
    "###The standardised_calcium_r12 line is looking better! Time to continue... (no need to use our last resort!)###\n",
    "\n",
    "### Last resort\n",
    "\n",
    "As a last resort, if a line cannot be fitted to the graph it is possible to remove that cell by:\n",
    "\n",
    "Removing the # below and replace [i] by the standardised_calcium_r[i] of interest.\n",
    "        \n",
    "This may reduce the acuracy of the overall average gradient for this cell type calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del average_gradient[i]\n",
    "\n",
    "# *No need to do this as all our lines are plotted quite nicely!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a07345",
   "metadata": {},
   "source": [
    "## Calculating the overall average gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b29533",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_decline_gradients = {} # Creating a dictionary for our average_decline_gradients for each standardised_calcium_r[i]\n",
    "\n",
    "# Looping through the variables and adding their average decline gradients to the dictionary\n",
    "for i in range(1, 51): # Looping through 50 possible columns\n",
    "    gradient = \"average_gradient\" + str(i) # Sets the variable to the one defined in the current iteration [i]\n",
    "    if gradient not in locals():\n",
    "        continue\n",
    "    average_decline_gradients[i] = locals()[gradient]\n",
    "\n",
    "\n",
    "# Defining a function to take an average of all the numbers in a dictionary.   \n",
    "def average_gradient(dictionary): # Defining the function with no arguments\n",
    "    total_sum = 0 # Setting our variables to 0\n",
    "    num_elements = 0 # Setting our variables to 0\n",
    "    for key, value in dictionary.items(): # Loops through each value\n",
    "        total_sum += value # Adds the value to the total value\n",
    "        num_elements += 1 # Adds 1 to the number of elements\n",
    "    return total_sum / num_elements, num_elements # Returns the total sum devided by the number of elements, and defines\n",
    "            # the number of elements\n",
    "\n",
    "# Using this function to calculate the overall average gradient from the average_decline_gradients dictionary.\n",
    "overall_average_gradient, num_elements = average_gradient(average_decline_gradients)\n",
    "\n",
    "print(\"Invidual cell gradients are:\\n\")\n",
    "for value in average_decline_gradients.values():\n",
    "    print(value)\n",
    "\n",
    "print(\"\\n\\nThe overall average gradient of calcium recovery of these\", num_elements, \"cells is: \\n\", overall_average_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5878e",
   "metadata": {},
   "source": [
    "# ### Success!\n",
    "### ### The avereage gradient can now be compared with other disease-states and analysed further! ###\n",
    "###Thank you for looking through my code!###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
